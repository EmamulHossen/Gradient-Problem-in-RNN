<div align="center">
      <h1> <img src="https://www.datasciencelearner.com/wp-content/uploads/2020/11/Vanishing-Gradient-Problem-in-RNN.png" width="80px"><br/>Gradient Problem in RNN</h1>
     </div>
<p align="center"> <a href="https://github.com/EmamulHossen" target="_blank"><img alt="" src="https://img.shields.io/badge/Website-EA4C89?style=normal&logo=dribbble&logoColor=white" style="vertical-align:center" /></a> <a href="https://www.facebook.com/emamul.hossen.503" target="_blank"><img alt="" src="https://img.shields.io/badge/Facebook-1877F2?style=normal&logo=facebook&logoColor=white" style="vertical-align:center" /></a> <a href="https://www.linkedin.com/in/emamul-hossen-9a8ab1255/}" target="_blank"><img alt="" src="https://img.shields.io/badge/LinkedIn-0077B5?style=normal&logo=linkedin&logoColor=white" style="vertical-align:center" /></a> </p>

# Description
 The vanishing gradient problem is a well-known issue in training recurrent neural networks (RNNs). It occurs when gradients (derivatives of the loss with respect to the network's parameters) become too small as they are backpropagated through the network during training. When the gradients vanish, it becomes difficult for the network to learn long-range dependencies, and the training process may slow down or even stagnate. This problem is especially pronounced in traditional RNN architectures.


 <img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/31_blog_image_2.png"> <img src="#"> <img src="#"> <img src="#"> <img src="#"> <br/>

 
**Basic Concept:**<br/>
**1.Weight**<br/>
**2.Bias**

### You can follow me:
[Links](https://www.facebook.com/emamul.hossen.503)
 
    
